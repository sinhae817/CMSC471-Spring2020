{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Chapter 4 - Local Search<br>Hill Climbing, Simulated Annealing,  <br>Gradient Descent & Genetic Algorithms</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Announcements, updates and reminders</b> \n",
    "- <b> Chapter 4: Local Search and Optimization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <b>Announcements, updates and reminders:</b>\n",
    "    - Quiz-2 is due Wed Feb 19.\n",
    "    - Quiz-3 is released on Wed Feb 19 and is due Wed Feb 26 11:59PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Assignment 1 \"Solving Problems by Searching\"](https://nbviewer.jupyter.org/github/fereydoonvafaei/CMSC471-Spring2020/blob/master/Assignment1/Assignment-1.ipynb) Due: Friday Feb 28th 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Schedule and reading assignments for Week 4 - this week (Feb 17 - Feb 21):\n",
    "    Chapter 4 \"Local Search\" and Chapter 5 \"Adversarial Search\".\n",
    "    - Schedule and reading assignments for Week 5 - next week (Feb 24 - Feb 28):\n",
    "    Chapter 6 \"Constraint Satisfaction Problems\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Major Announcement</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h1><center><font color=\"green\"> Midterm Exam: Wednesday March 25th</font></center></h1>\n",
    "\n",
    "<h2><center>Review Session: Monday March 23rd</center></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Chapter 4: Beyond Classical Search</center></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Local Search</center></h1>\n",
    "\n",
    "<h5><center>Methods Inspired by Statistical Physics (Simulated Annealing) and Evolutionary Biology (Genetic Algorithms)</center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Local Search</center></h1>\n",
    "\n",
    "- Local search algorithms operate using a single current node (rather than multiple paths), and generally move only to neighbors of that node.\n",
    "\n",
    "- Local serach algorithms are useful for solving <font color=\"blue\">optimization problems</font>, in which the aim is to find the best state according to an <font color=\"blue\">objective function</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Optimization Problems</center></h1>\n",
    "\n",
    "<img src=\"img/global-maximum.png\" align=\"center\"/>\n",
    "\n",
    "From Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "- Also known as Steepest Ascent: This is our first example of a local search algorithm.\n",
    "\n",
    "\n",
    "- Imagine you are climbing a mountain and you are in a very thick fog. You can only see a distance equal to one step length. To try to climb you take the step in the direction that is steepest to get to the highest point of all the locations you can currently see.\n",
    "\n",
    "\n",
    "- In other words, hill-climbing search simply evaluates the objective function for all states that are neighbors to the current state, and takes the neighbor state with the best objective function value as the new current state. If there are more than one next best states, one is picked randomly.\n",
    "\n",
    "\n",
    "- Hill-climbing search is sometimes called greedy search, because a step is taken after only considering the immediate neighbors. No time is spent considering possible future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "<img src=\"img/sa-1.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "<img src=\"img/hc.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing Failure</center></h1>\n",
    "\n",
    "<img src=\"img/hc-fail.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "Hill-climbing is easy to formulate and implement and often finds\n",
    "pretty good states quickly.  But, it has the following problems:\n",
    "\n",
    "  * it gets stuck on local optima (hills for maximizing searches, valleys for minimizing searches,\n",
    "  * it may get stuck on a ridge, if no single action can advance the search along the ridge,\n",
    "  * it may get stuck wandering on a plateau for which all neighboring states have equal value.\n",
    "\n",
    "Common variations include\n",
    "  * allow sideways moves (when on a plateau)\n",
    "  * stochastic hill-climbing: choose next state with probability related to increase in value of objective function\n",
    "  * first-choice hill-climbing: generate neighbors by random choice of available actions and keep first state that has better value,\n",
    "  * random-restart hill climbing: conduct multiple hill-climbing searches from multiple, randomly generated, initial states.\n",
    "\n",
    "Only this last one, with random-restarts, is **complete**.  In the limit, all states will be tried as starting states so the goal, or best state, will eventually be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing</center></h1>\n",
    "\n",
    "- Hill-climbing searches will get stuck on local optima.  Only by adding random restarts can you have a hill-climbing algorithm that is complete.\n",
    "\n",
    "\n",
    "- To get off of a local optimum, a search must be defined to allow steps that are \"downhill\" for maximizing searches, and \"uphill\" for minimizing searches, away from the optimum.  \n",
    "\n",
    "\n",
    "- [**Simulated annealing**](https://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf) is an algorithm that does this probabilisitically.  In metallurgy, **annealing** is the process used to temper or harden metals and glass by heating them to a high temprature and then gradually cooling them, thus allowing the material to reach a low-energy crystalline state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Assume we are doing a maximizing search, meaning we want to find the state with the maximum value.  Let the value of the current state be $v$.  Imagine an action has been applied to that state and the resulting state has a lower (worse) value $v'$.  Simulated annealing will accept this new state as the current state with probability: $$e^{(v' - v)/T}$$\n",
    "\n",
    "- $T$ is like a \"temperature\", the higher the value the more likely we are to take a step to a state with a worse value. In practice, $T$ starts at a high value and is slowly decreased towards zero.  If it is decreased \"slowly enough\", the global optimum will be found with probabilty 1.  In other words, this is a **complete** algorithm if the cooling strategy is slow enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing</center></h1>\n",
    "\n",
    "<img src=\"img/sa-1.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing - Ping-Pong Ball Shaking Intuition</center></h1>\n",
    "\n",
    "<img src=\"img/sa-ping-pong.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent</center></h1>\n",
    "\n",
    "<img src=\"img/gradient.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent - Learning Rate Too Small</center></h1>\n",
    "\n",
    "<img src=\"img/learning-rate-1.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent - Learning Rate Too Large</center></h1>\n",
    "\n",
    "<img src=\"img/learning-rate-2.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Optimization Demos](https://www.deeplearning.ai/ai-notes/optimization/?utm_source=social&utm_medium=linkedin&utm_campaign=BlogAINotesOptimizationAugust272019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>8-Queens Problem</center></h1>\n",
    "\n",
    "<img src=\"img/8queen.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Genetic Algorithm - 8 Queens</center></h1>\n",
    "\n",
    "<img src=\"img/genetic-8queen.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Genetic Algorithm - Inspired by Evolution Theory</center></h1>\n",
    "\n",
    "<img src=\"img/genetic.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Genetic Algorithm Demo -1](http://math.hws.edu/eck/js/genetic-algorithm/GA.html)\n",
    "\n",
    "[Geneti Algorithm Demo -2](https://www.youtube.com/watch?v=XcinBPhgT7M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Online Search I</center></h1>\n",
    "\n",
    "- \"An online search agent interleaves computation and action: first it takes an action, then it observes the environment and computes the next action.\"\n",
    "\n",
    "\n",
    "- \"Online search is a good idea in dynamic or semidynamic domains—domains where there is a penalty for sitting around and computing too long.\"\n",
    "\n",
    "\n",
    "- \"Online search is also helpful in nondeterministic domains because it allows the agent to focus its computational efforts on the contingencies that actually arise rather than those that might happen but probably won’t.\"\n",
    "\n",
    "All texts from Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Online Search II</center></h1>\n",
    "\n",
    "- \"Online search is a necessary idea for unknown environments, where the agent does not know what states exist or what its actions do. In this state of ignorance, the agent faces an exploration problem and must use its actions as experiments in order to learn enough to make deliberation worthwhile.\"\n",
    "\n",
    "\n",
    "- \"The canonical example of online search is a robot that is placed in a new building and must explore it to build a map that it can use for getting from A to B.\"\n",
    "\n",
    "\n",
    "- Another example for online learning is Learning Real Time A* (LRTA*) where heuristics are updated in the realtime - as oppsed to A-star where heuristic for the search space is provided offline and is static.\n",
    "\n",
    "\n",
    "- Two requirements for online search and learning: \n",
    "    - \"First, a formal and explicitly manipulable representation for general rules;\n",
    "\n",
    "    - Second, algorithms that can construct suitable general rules from the specific observations made by the agent.\"\n",
    "\n",
    "\n",
    "- To further imply this concept of online search, it's worth mentioning that in <b><font color=\"blue\">Reinforcement Learning</font></b>, there are usually two phases: <b>Exploration</b> - in which the agent explores the environment - and <b>Exploitation</b> - where the agent exploits the knowledge about the environment gained through Exploration.\n",
    "\n",
    "Quoted texts from Russel & Norvig Textbook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
