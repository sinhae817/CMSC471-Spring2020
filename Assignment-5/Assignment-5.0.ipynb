{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC471 - Artificial Intelligence - Spring 2020\n",
    "## Instructor: Fereydoon Vafaei\n",
    "# <font color=\"blue\"> Assignment 5: Classification and Regression Using NN in Tensorflow</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your name and ID here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part I of this assignment, you are going to build a neural network for binary classification. \n",
    "\n",
    "In Part II, you will build a NN for regression.\n",
    "\n",
    "<b>Note: </b>As you work through this assignment, you are recommended to check the textbook examples, notebooks and tensorflow documentations.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand classification and regression using Neural Networks.\n",
    "\n",
    "- practice implementing NNs in Tensorflow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Classification Using NN in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to build a binary classifier NN to predict disease, i.e. \"diagnosis\".\n",
    "\n",
    "The first thing to do is downloading [the breast cancer dataset](https://github.com/fereydoonvafaei/CMSC471-Spring2020/blob/master/Assignment-5/breast_cancer.csv) and save it in the same working directory as your notebook.\n",
    "\n",
    "Read the feature specifications in [Kaggle page](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/data) to learn more about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** As you work through the notebook, keep adding any required module from Python, Tensorflow and Keras in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from python, tensorflow and keras\n",
    "# NOTE: As you work through the notebook, keep adding any required module here if necessary\n",
    "...\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf Version:  2.1.0\n",
      "Eager Execution mode:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"tf Version: \", tf.__version__)\n",
    "print(\"Eager Execution mode: \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, load the data with pandas. The data (csv file) should be stored in the same working directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset using pd\n",
    "...\n",
    "\n",
    "# Show the first five rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check if there is any null or na in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Also, the first column `id` doesn't seem to provide any useful info to ML model, so drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"id\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, you can extract features and labels from `data`. Your classifier should attempt to predict `diagnosis` so that is your target/label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data to feature vector X and label vector y\n",
    "X = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (569, 30)\n",
      "Labels shape:  (569,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features shape: \", X.shape)\n",
    "print(\"Labels shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your `X` dataframe now only contains features, hence has 30 columns whereas `y` has now become a 1D vector containing labels only. Notice that `y` has 569 labels equal to the number of data records in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X should no longer contain the diagnosis which is target/label column - i.e. the column to be predicted\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    M\n",
       "1    M\n",
       "2    M\n",
       "3    M\n",
       "4    M\n",
       "Name: diagnosis, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y should only contain diagnosis - target/label column\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The two classes (aka labels) here are `M` and `B` representing `malignant` and `benign` which refers to the tumors you are going to classify. You need to represent them by `1` and `0` respectively. In other words, to use sklearn classifiers and score metrics, you need to convert the categorical lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical labels M and B to 1 and 0\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you have multiple features with different scales/ranges, you should consider standardizing them. There are different ways to standardize and to normalize the feature vector. One convenient way is using scikit-learn modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n",
       "         2.75062224,  1.93701461],\n",
       "       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n",
       "        -0.24388967,  0.28118999],\n",
       "       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n",
       "         1.152255  ,  0.20139121],\n",
       "       ...,\n",
       "       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n",
       "        -1.10454895, -0.31840916],\n",
       "       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n",
       "         1.91908301,  2.21963528],\n",
       "       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n",
       "        -0.04813821, -0.75120669]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (426, 30)\n",
      "y_train shape:  (426,)\n",
      "X_test shape:  (143, 30)\n",
      "y_test shape:  (143,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data to training set and testing set\n",
    "...\n",
    "\n",
    "# Check the shapes of X_train, X_test, y_train, y_test\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building NN for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, you should build a binary classifier NN that can predict diagnosis.\n",
    "\n",
    "> You can begin with a simple neural network with a couple of hidden layers, and increase number of hidden layers and neurons as needed. You may also use callback and early stopping to find the optimal number of epochs, but it's possible to obtain the minimum required accuracy (0.97) within 20 epochs only.\n",
    "\n",
    "> **Hint-1**: During training, despite some variations, you should see a clear trend of descending loss and increasing accuracy; otherwise, your model has not been developed properly.\n",
    "\n",
    "> **Hint-2**: Every time you want to train your network, you should start running the following cell to start with a fresh NN and to clear the previously trained weights. Otherwise, your results are not accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sequential NN with appropriate layers for binary classification of diagnosis\n",
    "# Use ReLU for all hidden layers\n",
    "\n",
    "# Hint1: input_dim of the first layer should match with the number of features in X_train\n",
    "\n",
    "# Hint2: Notice that the activation function and number of neurons in the output layer are determined\n",
    "# by the type of ML task i.e. Binary Classification\n",
    "nn_clf = tf.keras.Sequential([\n",
    "    # Add layers accordingly\n",
    "    ...\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, you should compile your `nn_clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile nn_clf with loss='binary_crossentropy' and metrics=['accuracy']\n",
    "# Hint1: One of the hyperparameters you can change is the optimizer (Adam, RMSprop, SGD, ...)\n",
    "# Hint2: The other impactful hyperparameter is learning_rate,\n",
    "# initially set it to 0.001 and fine-tune accordingly\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model using `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train nn_clf on X_train and y_train with 20 epochs\n",
    "nn_clf_history = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, plot the history of train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nn_clf_history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the xticks - label locations\n",
    "plt.xticks(np.arange(0, 20, step=2))  \n",
    "\n",
    "# set the y-axis range to [0-1]\n",
    "plt.gca().set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To evaluate the model, you use `evaluate()` method.\n",
    "\n",
    "> <font color='red'>**Minimum Accuracy Requirement**</font>: Your accuracy on `X_test` and `y_test` must be at least **0.97**. Otherwise, your notebook will get NO CREDIT for this part, so you must fine-tune your `nn_clf` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on X_test and y_test\n",
    "loss, accuracy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimum Required Accuracy: 0.97\n",
    "round(accuracy, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Regression Using NN in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you build a regression model for prediction of video game sales using NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset directly from [here](https://github.com/fereydoonvafaei/CMSC471-Spring2020/blob/master/Assignment-5/video-games.csv).\n",
    "\n",
    "The description of the dataset you're going to work on can be seen [here](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings). You are going to use the features to predict video game sales in Europe `EU_Sales`.\n",
    "\n",
    "Follow the instructions for loading the data, and preprocessing very carefully. Even though preprocessing has 10 points only, if you don't do it correctly, your whole results would be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as a dataframe using pandas\n",
    "game_data = ...\n",
    "print(game_data.shape)\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "...\n",
    "print(game_data.shape)\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Name\" column as it does not provide any useful info\n",
    "...\n",
    "print(game_data.shape)\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Global_Sales\" column as it is redundant feature - it's just sum of regional and other sales\n",
    "...\n",
    "print(game_data.shape)\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vector X_reg (all columns but \"EU_Sales\") and target label y_reg as \"EU_Sales\"\n",
    "X_reg = ...\n",
    "y_reg = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print X_reg shape and head\n",
    "print(X_reg.shape)\n",
    "X_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas.get_dummies() create dummy variables for categorical features of X_reg\n",
    "...\n",
    "print(X_reg.shape)\n",
    "X_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Note:</b> The output of the following cells is provided to you for your reference. All the following cells depend on the correctness of your preprocessing steps and can be verified by these outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 1683)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Platform_3DS</th>\n",
       "      <th>Platform_DC</th>\n",
       "      <th>Platform_DS</th>\n",
       "      <th>...</th>\n",
       "      <th>Developer_odenis studio</th>\n",
       "      <th>Developer_syn Sophia</th>\n",
       "      <th>Developer_zSlide</th>\n",
       "      <th>Rating_AO</th>\n",
       "      <th>Rating_E</th>\n",
       "      <th>Rating_E10+</th>\n",
       "      <th>Rating_K-A</th>\n",
       "      <th>Rating_M</th>\n",
       "      <th>Rating_RP</th>\n",
       "      <th>Rating_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.341176</td>\n",
       "      <td>42.346639</td>\n",
       "      <td>12.886767</td>\n",
       "      <td>31.004904</td>\n",
       "      <td>0.413014</td>\n",
       "      <td>1.147975</td>\n",
       "      <td>0.250716</td>\n",
       "      <td>-0.15243</td>\n",
       "      <td>-0.045334</td>\n",
       "      <td>-0.270063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.01712</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>1.509226</td>\n",
       "      <td>-0.397162</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.515485</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.730971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.133743</td>\n",
       "      <td>15.800856</td>\n",
       "      <td>12.956315</td>\n",
       "      <td>11.884655</td>\n",
       "      <td>0.845647</td>\n",
       "      <td>2.292368</td>\n",
       "      <td>0.909519</td>\n",
       "      <td>-0.15243</td>\n",
       "      <td>-0.045334</td>\n",
       "      <td>-0.270063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.01712</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>1.509226</td>\n",
       "      <td>-0.397162</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.515485</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.730971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.371202</td>\n",
       "      <td>15.728496</td>\n",
       "      <td>11.182831</td>\n",
       "      <td>10.624793</td>\n",
       "      <td>0.701436</td>\n",
       "      <td>2.292368</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>-0.15243</td>\n",
       "      <td>-0.045334</td>\n",
       "      <td>-0.270063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.01712</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>1.509226</td>\n",
       "      <td>-0.397162</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.515485</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.730971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.341176</td>\n",
       "      <td>11.252514</td>\n",
       "      <td>22.380122</td>\n",
       "      <td>10.365410</td>\n",
       "      <td>1.350385</td>\n",
       "      <td>1.876225</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>-0.15243</td>\n",
       "      <td>-0.045334</td>\n",
       "      <td>3.702302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.01712</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>1.509226</td>\n",
       "      <td>-0.397162</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.515485</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.730971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.341176</td>\n",
       "      <td>14.022868</td>\n",
       "      <td>9.965734</td>\n",
       "      <td>10.217191</td>\n",
       "      <td>-0.884885</td>\n",
       "      <td>0.627797</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>-0.15243</td>\n",
       "      <td>-0.045334</td>\n",
       "      <td>-0.270063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.01712</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>1.509226</td>\n",
       "      <td>-0.397162</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.515485</td>\n",
       "      <td>-0.012105</td>\n",
       "      <td>-0.730971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1683 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Release   NA_Sales   JP_Sales  Other_Sales  Critic_Score  \\\n",
       "0        -0.341176  42.346639  12.886767    31.004904      0.413014   \n",
       "2         0.133743  15.800856  12.956315    11.884655      0.845647   \n",
       "3         0.371202  15.728496  11.182831    10.624793      0.701436   \n",
       "6        -0.341176  11.252514  22.380122    10.365410      1.350385   \n",
       "7        -0.341176  14.022868   9.965734    10.217191     -0.884885   \n",
       "\n",
       "   Critic_Count  User_Count  Platform_3DS  Platform_DC  Platform_DS  ...  \\\n",
       "0      1.147975    0.250716      -0.15243    -0.045334    -0.270063  ...   \n",
       "2      2.292368    0.909519      -0.15243    -0.045334    -0.270063  ...   \n",
       "3      2.292368    0.029412      -0.15243    -0.045334    -0.270063  ...   \n",
       "6      1.876225    0.436270      -0.15243    -0.045334     3.702302  ...   \n",
       "7      0.627797   -0.077835      -0.15243    -0.045334    -0.270063  ...   \n",
       "\n",
       "   Developer_odenis studio  Developer_syn Sophia  Developer_zSlide  Rating_AO  \\\n",
       "0                -0.012105              -0.01712         -0.012105  -0.012105   \n",
       "2                -0.012105              -0.01712         -0.012105  -0.012105   \n",
       "3                -0.012105              -0.01712         -0.012105  -0.012105   \n",
       "6                -0.012105              -0.01712         -0.012105  -0.012105   \n",
       "7                -0.012105              -0.01712         -0.012105  -0.012105   \n",
       "\n",
       "   Rating_E  Rating_E10+  Rating_K-A  Rating_M  Rating_RP  Rating_T  \n",
       "0  1.509226    -0.397162   -0.012105 -0.515485  -0.012105 -0.730971  \n",
       "2  1.509226    -0.397162   -0.012105 -0.515485  -0.012105 -0.730971  \n",
       "3  1.509226    -0.397162   -0.012105 -0.515485  -0.012105 -0.730971  \n",
       "6  1.509226    -0.397162   -0.012105 -0.515485  -0.012105 -0.730971  \n",
       "7  1.509226    -0.397162   -0.012105 -0.515485  -0.012105 -0.730971  \n",
       "\n",
       "[5 rows x 1683 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize X_reg using mean() and std()  NOTE: The output is provided for your reference.\n",
    "...\n",
    "print(X_reg.shape)\n",
    "X_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5460, 1683)\n",
      "(5460,)\n",
      "(1365, 1683)\n",
      "(1365,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data to train and test with ratio of 80/20 for train/test respectively\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = ...\n",
    "print(X_reg_train.shape)\n",
    "print(y_reg_train.shape)\n",
    "print(X_reg_test.shape)\n",
    "print(y_reg_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building NN for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are recommended to try different architectures (different number of hidden layers and neurons) to get the desired loss. You may start with a couple of hidden layers and a few neurons and add accordingly until you hit below the maximum acceptable loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sequential NN with appropriate layers for regression to predict EU_Sales\n",
    "# Use ReLU for all hidden layers\n",
    "\n",
    "# Hint1: input_dim of the first layer should match with the number of features in X_reg_train\n",
    "# Hint2: Notice that the activation function and number of neurons in the output layer are determined\n",
    "# by the type of ML task i.e. Regression\n",
    "\n",
    "nn_reg = tf.keras.Sequential([\n",
    "    # Add layers accordingly\n",
    "    \n",
    "    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile nn_reg: set both loss and metric to 'mse',\n",
    "# and set the optimizer to 'RMSprop' with a learning rate of 0.001\n",
    "\n",
    "# For this regression task, loss and metrics are the same\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network on X_reg_train and y_reg_train with 20 epochs\n",
    "nn_reg_history = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nn_reg_history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the xticks - label locations\n",
    "plt.xticks(np.arange(0, 20, step=2))  \n",
    "\n",
    "# set the y-axis range to [0-1]\n",
    "plt.gca().set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='red'>**Maximum Acceptable MSE Loss Requirement**</font>: The MSE loss of your model evaluated on `X_reg_test` and `y_reg_test` should not exceed **0.20**. Otherwise, your notebook will get NO CREDIT for this part, so you must fine-tune your `nn_reg` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on X_reg_test, y_reg_test\n",
    "mse, mse = nn_reg.evaluate(X_reg_test, y_reg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum acceptable mse loss: 0.20\n",
    "round(mse, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Assignment-5 has a maximum of 100 points. Make sure that you get the correct outputs and plots for all cells that you implement and give complete answers to all questions. Also, your notebook should be written with no grammatical and spelling errors and should be nicely-formatted and easy-to-read.\n",
    "\n",
    "Notice that even though preprocessing has 10 points, if your preprocessing steps are not correct, your notebook will get ZERO because all the results would be wrong, so be very careful with preprocessing.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "- Part I Classification on diagnosis: [total 50 points]\n",
    "    - Preprocessing: 10 points\n",
    "    - Implementation of nn_clf: 40 points - **Minimum Required Accuracy**: 0.97 otherwise ZERO CREDIT!\n",
    "    \n",
    "- Part II Regression on NA_Sales: [total 50 points]\n",
    "    - Preprocessing: 10 points\n",
    "    - Implementation of nn_reg: 40 points - **Maximum Acceptable MSE Loss**: 0.20 otherwise ZERO CREDIT!\n",
    "   \n",
    "\n",
    "Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical, spelling or formatting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Name your notebook ```Lastname-A5.ipynb```. Submit the file using the ```Assignment-5``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct implementation, correct results and plots, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Tuesday May 12th, 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
