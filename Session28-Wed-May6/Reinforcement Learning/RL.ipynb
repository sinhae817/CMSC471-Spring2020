{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Introduction to Reinforcement Learning</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b>Reinforcement Learning</b>\n",
    "    - Introduction\n",
    "    - Applications\n",
    "    - Terminology\n",
    "        - Rewards\n",
    "        - Actions\n",
    "        - States\n",
    "        - Reinforcement\n",
    "        - Return\n",
    "        - Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL vs Supervised/Unsupervised Learning</center></h1>\n",
    "\n",
    "- Like supervised and unsupervised learning, reinforcement learning is one of the most exciting fields of Machine Learning.\n",
    "\n",
    "\n",
    "- Reinforcement learning is different from other ML paradigms as summarized below: \n",
    "\n",
    "    - feedback is only provided as the reward signal,\n",
    "    - the feedback can be delayed, \n",
    "    - data is fed sequentially, \n",
    "    - there is interaction based on the actions taken. \n",
    "\n",
    "\n",
    "- The reinforcement learning resembles huuman learning (in the context of neuroscience) or animal training that treats reward good behavior.\n",
    "\n",
    "\n",
    "- When the series of actions end up with good results, those actions can be **reinforced** by giving some rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL Applications</center></h1>\n",
    "\n",
    "<center><img src=\"img/rl-apps.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL: Actions and Rewards</center></h1>\n",
    "\n",
    "- In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives **rewards**.\n",
    "\n",
    "\n",
    "- Its objective is to learn to act in a way that will **maximize** its expected **rewards** over time.\n",
    "\n",
    "\n",
    "- The agent acts in the environment and learns by trial and error to maximize its **positive rewards** (pleasure) and minimize its **negative rewards** (pain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL Concepts</center></h1>\n",
    "\n",
    "- Imagine a position in a tic-tac-toe game (knots and crosses).\n",
    "\n",
    "\n",
    "- How do you choose the best  next action?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/ttt1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL Concepts</center></h1>\n",
    "\n",
    "- Which are you most likely to win from?\n",
    "\n",
    "\n",
    "- Guess at how likely you are  to win from each state.  Is a win definite, likely, or maybe?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/ttt2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>RL Terminology</center></h1>\n",
    "\n",
    "- Reward: $R_t$ is a scalar feedback signal at time $t$. It indicates how well agent is doing at the time. \n",
    "\n",
    "- State: $S_t$ represents what happens currently and next. \n",
    "\n",
    "- Action: $A_t$ is how an agent affects to the environment that can change the state. \n",
    "\n",
    "- Observation: $O_t$ is what an agent recognizes the world for the state $S_t$. \n",
    "\n",
    "- Policy: A function that determines an agent's behavior or a function that selects its action, $\\pi(S_t)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>States</center></h1>\n",
    "\n",
    "Set of possible states, $\\mathcal{S}$.\n",
    "\n",
    "   * Can be discrete values ($|\\mathcal{S}| < \\infty$)\n",
    "       * Tic-Tac-Toe game positions\n",
    "       * Position in a maze\n",
    "       * Sequence of steps in a plan\n",
    "   *  Can be continuous values ($|\\mathcal{S}| = \\infty$)\n",
    "       * Joint angles of a robot arm\n",
    "       * Position and velocity of a race car\n",
    "       * Parameter values for a network routing strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Actions</center></h1>\n",
    "\n",
    "Set of possible actions, $\\mathcal{A}$.\n",
    "\n",
    "   * Can be discrete values ($|\\mathcal{A}| < \\infty$)\n",
    "       *  Next moves in Tic-Tac-Toe \n",
    "       * Directions to step in a maze\n",
    "       * Rearrangements of a sequence of steps in a plan\n",
    "   * Can be continuous values ($|\\mathcal{A}| = \\infty$)\n",
    "       * Torques to apply to the joints of a robot arm\n",
    "       *  Fuel rate and turning torque in a race car\n",
    "       * Settings of parameter values for a network routing strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Robot Walking</center></h1>\n",
    "\n",
    "- What can be state?\n",
    "- What can be action?\n",
    "- What is reward? \n",
    "- What are the observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors.\n",
    "\n",
    "\n",
    "- It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stock Trading</center></h1>\n",
    "\n",
    "- What can be state?\n",
    "- What can be action?\n",
    "- What is reward? \n",
    "- What are the observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent can observe stock market prices and decide how much to buy or sell every second.\n",
    "\n",
    "\n",
    "- Rewards are obviously the monetary gains and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Values</center></h1>\n",
    "\n",
    "- We want to choose the action that we predict will result in the best possible future from the current state. Need a value that represents the future outcome.\n",
    "\n",
    "\n",
    "- What should the value represent?\n",
    "\n",
    "    - Tic-Tac-Toe: Likelihood of winning from a game position.\n",
    "    - Maze: Number of steps to reach the goal.\n",
    "    - Plan: Efficiency in time and cost of accomplishing the objective for particular rearrangment of steps in a plan.\n",
    "    - Robot: Energy required to move the gripper on a robot arm to a destination.\n",
    "    - Race car: Time to reach the finish line.\n",
    "    - Network routing: Throughput.\n",
    "\n",
    "\n",
    "- With the correct values, multi-step decision problems are reduced to single-step decision problems. Just pick action with best value. Guaranteed to find optimal multi-step solution (dynamic programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Reinforcement and Return</center></h1>\n",
    "\n",
    "- The utility or cost of a single action taken from a state is the **reinforcement** for that action from that state. The value of that state-action is the expected value of the full **return** or the sum of reinforcements that will follow when that action is taken.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/returns1.png\">\n",
    "\n",
    "- Say this continues until we reach a goal state, $K$ steps later. What is the return $R_t$ from state $s_t$?\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      R_t = \\sum_{k=0}^K r_{t+k+1}\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Choosing the Best Action</center></h1>\n",
    "\n",
    "- Use the returns to choose the best action.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/returns2.png\">\n",
    "\n",
    "- Are we maximizing or minimizing?  What does the reinforcement represent?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's say it is energy used that we want to minimize.  $a_1$, $a_2$, or $a_3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Negative Rewards</center></h1>\n",
    "\n",
    "- Note that there may not be any **positive rewards** at all; for example, the agent may move around in a maze, getting a **negative reward** at every time step, so it had better find the exit as quickly as possible!\n",
    "\n",
    "\n",
    "- There are many other examples of tasks to which Reinforcement Learning is well suited, such as self-driving cars, recommender systems, placing ads on a web page, or controlling where an image classification system should focus its attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Where Do the Values Come From?</center></h1>\n",
    "\n",
    "   * Write the code to calculate them.    \n",
    "      * Usually not possible. If you can do this for your problem, why are you considering machine learning? Might be able to do this for Tic-Tac-Toe.\n",
    "      \n",
    "      \n",
    "   * Use dynamic programming.\n",
    "      *   Usually not possible. Requires knowledge of the probabilities of transitions between all states for all actions. \n",
    "      \n",
    "      \n",
    "   * Learn from examples, lots of examples. Lots of 5-tuples: state, action, reinforcement, next state, next action ($s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$).\n",
    "      *  **Monte Carlo:** Assign to each state-action pair an average of the observed returns: $ \\;\\;\\;\\text{value}(s_t,a_t) \\approx \\text{mean of } R(s_t,a_t)$\n",
    "      *  **Temporal Difference (TD):** Using $\\text{value}(s_{t+1},a_{t+1})$ as estimate of return from next state, update current state-action value: $\\;\\;\\; \\text{value}(s_t,a_t) \\approx r_{t+1} + \\text{value}(s_{t+1},a_{t+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Neural Network Policy</center></h1>\n",
    "\n",
    "- The algorithm a software agent uses to determine its actions is called its **policy**.\n",
    "\n",
    "\n",
    "- The **policy** could be a neural network taking observations as inputs and outputting the action to take.\n",
    "\n",
    "<center><img src=\"img/nn-policy.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Policy Space and Policy Search</center></h1>\n",
    "\n",
    "- How would you train a vacuum cleaner robot whose reward is the amount of dust it picks up in 30 minutes?\n",
    "\n",
    "- There are just two policy parameters you can tweak: the probability p and the angle range r.\n",
    "\n",
    "- One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best which would be an example of policy search, in this case using a brute force approach.\n",
    "\n",
    "<center><img src=\"img/policy-space.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "\n",
    "# in CartPole, each observation is a 1D NumPy array containing four floats: these floats represent:\n",
    "# the cart’s horizontal position ( 0.0 = center), its velocity (positive means right),\n",
    "# the angle of the pole ( 0.0 = vertical), and its angular velocity (positive means clockwise).\n",
    "# \n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CartPole Environment</center></h1>\n",
    "\n",
    "<center><img src=\"img/cartpole.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Exploration vs Exploitation</center></h1>\n",
    "\n",
    "- You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score.\n",
    "\n",
    "\n",
    "- This approach lets the agent find the right balance between **exploring** new actions and **exploiting** the actions that are known to work well.\n",
    "\n",
    "\n",
    "- Here’s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing, so you randomly pick one. If it turns out to be good, you can increase the probability that you’ll order it next time, but you shouldn’t increase that probability up to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Neural Network Policy</center></h1>\n",
    "\n",
    "- Let’s create a neural network policy. Just like with the policy we hardcoded earlier, this neural network will take an observation as input, and it will output the action to be executed.\n",
    "\n",
    "- More precisely, it will estimate a probability for each action, and then we will select an action randomly, according to the estimated probabilities.\n",
    "\n",
    "- In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probabil ity p of action 0 (left), and of course the probability of action 1 (right) will be 1 – p.\n",
    "\n",
    "<center><img src=\"img/nn-policy-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019\n",
    "\n",
    "[2] Professor Chuck Anderson's Notebooks from CSU\n",
    "\n",
    "[3] MJ Lee's Slides from UNCC"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
