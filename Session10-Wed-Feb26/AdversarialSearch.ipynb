{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Chapter 5 - Adversarial Search<br>\"Games\"</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Chapter 5: Adversarial Search</center></h1>\n",
    "\n",
    "<h5><center>Also Known as Games</center></h5>\n",
    "\n",
    "<h7><center>\"In which we examine the problems that arise when we try to plan ahead in a world where other agents are planning against us.\"</center></h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Assumptions: Zero-Sum and Perfect Information</center></h1>\n",
    "\n",
    "- Initially, we focus on games that are deterministic and completely observable.\n",
    "\n",
    "- We also assume that the payoff to each player at the end of a game is equal and opposite, called **zero-sum**.\n",
    "\n",
    "- Moreover, the two players both have access to complete information about the states that may lead to a better move and eventually win/loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Definition of Game</center></h1>\n",
    "\n",
    "  * initial state $s_0$\n",
    "  * $player(s)$: which player is to move in state $s$,\n",
    "  * $actions(s)$: legal actions from state $s$,\n",
    "  * $result(s,a)$: state that results, like our `takeActionF` in Assignment-1\n",
    "  * $terminalTest(s)$: true when game is over\n",
    "  * $utility(s,p)$: payoff for player $p$ upon reaching state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax</center></h1>\n",
    "\n",
    "The two players in a two person game will be called `Max` and\n",
    "`Min`. These names reflect the meaning of the $utility(s,p)$ \n",
    "function, which is to be maximized by Player `Max` and minimized by\n",
    "Player `Min`. \n",
    "\n",
    "The partial search tree in the next slide illustrates the\n",
    "reasoning behind the concept of alternate layers minimizing and\n",
    "maximizing the utility value to back up a value from terminal states\n",
    "to non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/minmax-example.png\" align=\"center\"/></center>\n",
    "\n",
    "From Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>MiniMax</center></h1>\n",
    "\n",
    "The calculation of the `minimax(s)` value of a state $s$ can be\n",
    "summarized as\n",
    "\n",
    "$$\n",
    "\\text{minimax}(s) = \\begin{cases}\n",
    "utility(s), & \\text{if }terminalTest(s);\\\\\n",
    "\\max_{a\\in actions(s)} \\text{minimax}(result(s,a)), & \\text{if\n",
    "}player(s) \\text{ is Max};\\\\\n",
    "\\min_{a\\in actions(s)} \\text{minimax}(result(s,a)), & \\text{if\n",
    "}player(s) \\text{ is Min}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Assumes player `Min` plays optimally.  If not, `Max` will do even\n",
    "better.\n",
    "\n",
    "The textbook shows in Figure 5.3 the *minimax-decision* algorithm as\n",
    "a depth-first search that altenates between calling `max-value` and\n",
    "`min-value` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"minmax.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3bf40cd898>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"minmax.pdf\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning</center></h1>\n",
    "\n",
    "Some of the search tree can be ignored if we know we cannot find a\n",
    "better move from the best one found so far.  If you are Player X in\n",
    "Tic-Tac-Toe, and\n",
    "  * your best move so far will result in a draw, and\n",
    "  * the next move you are evaluating you discover your opponent can definitely win from,\n",
    "  * do not explore any other choices your opponent might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning</center></h1>\n",
    "\n",
    "For each node, keep track of three values, Minimax value (the same value returned by Minimax algorithm), as well as $\\alpha$ and $\\beta$\n",
    "\n",
    "$\\alpha$ is best value by any means\n",
    "  * Any value less than this is no use because we already now how to achieve at least a value of $\\alpha$\n",
    "  * $\\alpha = Max(current value, new value)$\n",
    "  * Initially, $- \\infty$\n",
    "  * $\\alpha$ is updated only at Max nodes\n",
    "\n",
    "$\\beta$ is worst value for the opponent\n",
    "  * $\\beta = Min(current value, new value)$\n",
    "  * Initially, $+ \\infty$\n",
    "  * $\\beta$ is updated only at Min nodes\n",
    "  \n",
    "The span between $\\alpha$ and $\\beta$ progressively gets smaller.\n",
    "\n",
    "Any unvisited children/subtree of the node for which $\\beta <= \\alpha$ can be pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Alpha-Beta Pruning - Example</center></h1>\n",
    "\n",
    "http://inst.eecs.berkeley.edu/~cs61b/fa14/ta-materials/apps/ab_tree_practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"alpha-beta-example.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3bf40cde48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"alpha-beta-example.pdf\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Functions</center></h1>\n",
    "\n",
    "- Used to evaluate the \"goodness\" of a game position\n",
    "\n",
    "- Contrast with heuristic search, where evaluation function is an estimate of cost from start node to goal passing through given node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Functions</center></h1>\n",
    "\n",
    "- $f(n) >> 0$ position n good for me; bad for you\n",
    "\n",
    "- $f(n) << 0$ position n good for you; bad for me\n",
    "\n",
    "- $f(n)$ near $0$ position n is a neutral position\n",
    "\n",
    "- $f(n) = +\\infty$ position n win for me\n",
    "\n",
    "- $f(n) = -\\infty$ position n win for you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Functions Example - Chess</center></h1>\n",
    "\n",
    "- Claude Shannon's paper *Programming a Computer for Playing Chess (1950)* was among the first proposals to apply evaluation functions to states in the search.\n",
    "\n",
    "\n",
    "- Alan Turing’s function for chess:\n",
    "- $f(n) = w(n) / b(n)$ where $w(n)$ is sum of point value of white’s pieces and $b(n)$ is black's pieces.\n",
    "\n",
    "\n",
    "- Traditional piece values: pawn:1; knight:3; bishop:3; rook:5; queen:9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function Formulation</center></h1>\n",
    "\n",
    "$$Eval(s) = w_1f_1(s) + w_2f_2(s) + ... w_nf_n(s) = \\sum_{i=1}^n w_i f_i$$\n",
    "\n",
    "where each $w_i$ is a weight and each $f_i$ is a feature of the position. For chess, $f_i$ could be the numbers of each kind of piece on the board and the $w_i$ could be the values of the pieces.\n",
    "\n",
    "- IBM’s chess program [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)) (circa 1996) had $>8K$ features in its evaluation function.\n",
    "\n",
    "- In DeepBlue’s alpha-beta pruning, average branching factor at node was ~6 instead of ~35! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>IBM Deep Blue</center></h1>\n",
    "\n",
    "<center><img src=\"img/deepblue.jpg\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function Estimation</center></h1>\n",
    "\n",
    "- In games where not so much experience is available like chess, the weights of the evaluation function can be estimated by the machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stochastic Games</center></h1>\n",
    "\n",
    "<center><img src=\"img/stochastic-games.png\" align=\"center\"/></center>\n",
    "\n",
    "Image from Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Adversarial Search Summary</center></h1>\n",
    "\n",
    "- Games Assumptions: Zero-Sum and Perfect Information\n",
    "\n",
    "- Definition of Games\n",
    "\n",
    "- Minimax\n",
    "\n",
    "- Alpha-Beta Pruning\n",
    "\n",
    "- Evaluation Functions\n",
    "\n",
    "- Stochastic Games"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
